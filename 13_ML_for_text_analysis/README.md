# Описание проекта

Один интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.
Значение метрики качества *F1* модели должно быть не меньше 0.75. 

# Описание данных

В нашем распоряжении набор данных с разметкой о токсичности правок:

- `text` - текст комментария;
- `toxic` - целевой признак (1-комментарий токсичен, 0-нетоксичен)

# Ход исследования

## Анализ и предобработка данных:
- проверка на пропуски;
- проверка соотношения классов токсичных и нетоксичных комментариев;
- лемматизация комментариев и очистка текстов от лишних символов (для лемматизации англоязычных текстов используется WordNetLemmatizer);
- преобразование корпуса в мешок слов и очистка текстов от стоп-слов, лишенных смысловой нагрузки;
- выбор способа для баланса классов.

## Обучение нескольких моделей (Логистической регрессии, CatBoost lassifier, SGDClassifier) и сравнение метрик качества моделей.

# Стэк
Re, Pandas, Numpy, Matplotlib, nltk (WordNetLemmatizer, nltk_stopwords), Scikit-learn.

# Выводы

В рамках проекта мы опробовали для решения задачи классификации три модели:

- Логистическую регрессию;
- CatBoost Classifier;
- SGD Classifier.

В исходных данных наблюдался большой дисбаланс классов, но опытным путем было установлено, что наилучший результат дает способ с изменением весов модели.

Все три модели на тестовой выборке прошли пороговое значение метрики F1 в 0.75, но:

На тестовой выбоке по метрике F1 лучше всего себя показала LogisticRegression. Данная модель обладает бОльшими показателями Precision и Accuracy. Это говорит нам, что токсичные комментарии находятся лучше.
У SGDClassifier выше метрика ROC-AUC и Recall.
CatBoost из всех трех моделей дал худшие показатели.
